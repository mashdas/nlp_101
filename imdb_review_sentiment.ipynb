{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "imdb_review_sentiment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN7qAzQKSrhbtLoQ3tUyDrn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mashdas/nlp_101/blob/master/imdb_review_sentiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tw5xqYPRvaCF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Flatten,Dense,GlobalMaxPooling1D,Embedding\n",
        "from tensorflow.keras import Sequential"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lQkoRpTwxIE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow_datasets as tfds\n",
        "imdb,info=tfds.load(\"imdb_reviews\",with_info=True,as_supervised=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wQFwe0hv-Cz",
        "colab_type": "code",
        "outputId": "7abf0323-0641-418c-ba81-ecdf4e5d2c25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "imdb"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'test': <DatasetV1Adapter shapes: ((), ()), types: (tf.string, tf.int64)>,\n",
              " 'train': <DatasetV1Adapter shapes: ((), ()), types: (tf.string, tf.int64)>,\n",
              " 'unsupervised': <DatasetV1Adapter shapes: ((), ()), types: (tf.string, tf.int64)>}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EE1NzMwwlN2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data,test_data=imdb[\"train\"],imdb[\"test\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZES4YgYXxiqE",
        "colab_type": "code",
        "outputId": "c5a0a782-127d-4672-98d0-e96ec2597109",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "for x in train_data:\n",
        "  print(len(x))\n",
        "  break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGjPxqD-xj1U",
        "colab_type": "code",
        "outputId": "de2c46b6-259c-4b37-eddb-f606c9342b00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "a,b=None,None\n",
        "for x,y in train_data:\n",
        "  print(x)\n",
        "  print(y)\n",
        "  a,b=x,y\n",
        "  break\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\", shape=(), dtype=string)\n",
            "tf.Tensor(0, shape=(), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1KdNTyMzIRC",
        "colab_type": "code",
        "outputId": "5ad3661c-9cde-46d4-919c-dce262bcc93e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print(type(a))\n",
        "print(b)\n",
        "##a and b are both tensors..Need to convert them to the proper types\n",
        "##before tokenizing and training the model"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
            "tf.Tensor(0, shape=(), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0i4YwuHtzJAS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_sentences,training_labels=[],[]\n",
        "testing_sentences,testing_labels=[],[]\n",
        "for x,y in train_data:\n",
        "  training_sentences.append(str(x.numpy()))\n",
        "  training_labels.append(int(y))\n",
        "\n",
        "for x,y in test_data:\n",
        "  testing_sentences.append(str(x.numpy()))\n",
        "  testing_labels.append(int(y))\n",
        "\n",
        "training_labels,testing_labels=np.array(training_labels),np.array(testing_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5L5lL-efzaP1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##Building the vocabulary and vectors\n",
        "\n",
        "tokenizer=Tokenizer(num_words=10000,oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(training_sentences)\n",
        "word_index=tokenizer.word_index\n",
        "sequences=tokenizer.texts_to_sequences(training_sentences)\n",
        "padded=pad_sequences(sequences,maxlen=140,truncating=\"post\")\n",
        "\n",
        "test_seq=tokenizer.texts_to_sequences(testing_sentences)\n",
        "testing_padded=pad_sequences(test_seq,maxlen=140)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m64ZDH0b4DU0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##Building the model\n",
        "max_len=140\n",
        "embedding_dim=16\n",
        "vocab_size=10000\n",
        "model=Sequential()\n",
        "model.add(Embedding(vocab_size,embedding_dim,input_length=max_len))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(6,activation=\"relu\"))\n",
        "model.add(Dense(1,activation=\"sigmoid\"))\n",
        "\n",
        "model.compile(loss=\"binary_crossentropy\",optimizer=\"adam\",metrics=[\"accuracy\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utw0QT5X5z9a",
        "colab_type": "code",
        "outputId": "84c7ee34-5b82-4e3d-9a68-e15659ba6165",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 140, 16)           160000    \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 2240)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 6)                 13446     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 7         \n",
            "=================================================================\n",
            "Total params: 173,453\n",
            "Trainable params: 173,453\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "250cYhtc7mNh",
        "colab_type": "code",
        "outputId": "22dc9e0d-a17c-454e-daed-df52344d66d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        }
      },
      "source": [
        "model.fit(padded, training_labels, e/pochs=10, validation_data=(testing_padded, testing_labels))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 0.5044 - accuracy: 0.7235 - val_loss: 0.3362 - val_accuracy: 0.8522\n",
            "Epoch 2/10\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 0.2352 - accuracy: 0.9104 - val_loss: 0.3544 - val_accuracy: 0.8482\n",
            "Epoch 3/10\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 0.0939 - accuracy: 0.9755 - val_loss: 0.4401 - val_accuracy: 0.8359\n",
            "Epoch 4/10\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 0.0273 - accuracy: 0.9958 - val_loss: 0.4991 - val_accuracy: 0.8374\n",
            "Epoch 5/10\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 0.0097 - accuracy: 0.9989 - val_loss: 0.5431 - val_accuracy: 0.8403\n",
            "Epoch 6/10\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 0.0032 - accuracy: 0.9998 - val_loss: 0.5949 - val_accuracy: 0.8392\n",
            "Epoch 7/10\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.6489 - val_accuracy: 0.8387\n",
            "Epoch 8/10\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 5.3905e-04 - accuracy: 1.0000 - val_loss: 0.6785 - val_accuracy: 0.8402\n",
            "Epoch 9/10\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 2.8701e-04 - accuracy: 1.0000 - val_loss: 0.7161 - val_accuracy: 0.8406\n",
            "Epoch 10/10\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 1.7129e-04 - accuracy: 1.0000 - val_loss: 0.7542 - val_accuracy: 0.8404\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fbcb4696828>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5z_5nQF57xT0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##Totally overfitting\n",
        "##TRY TWEAKING THE VOCAB_SIZE AND PADDING_SIZE...(REDUCE IT PROBABLY)\n",
        "##Try using Subwords,RNN and LSTM in conjuncion\n",
        "\n",
        "\n",
        "##Now..to see the word vectors in the embedding space\n",
        "##Word_index gives us a dictionary with elements as word:some_number\n",
        "##We need to create a new dict/list that gives us some_number:word\n",
        "word_index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dMWri718rsv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reverse_word_index=dict([(x,y) for (y,x) in word_index.items()])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JV3JUhD_-PMF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reverse_word_index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSOUjMIJ8xIv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##As we will be iterating over our review,thats now encoded,\n",
        "##we need to create afunction that takes in an array and outputs the \n",
        "##original human-understandable review\n",
        "def decode_review(text):\n",
        "    return ' '.join([reverse_word_index.get(i, '?') for i in text])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JF3RW7oZ_TlR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "l=model.layers[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QYmKZDk_axn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weights=l.get_weights()[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULO3Eho7_bgw",
        "colab_type": "code",
        "outputId": "d721b7c5-ccdb-45a3-fd46-e2d3ed306fd2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 16)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q10uNmkd__Cl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##Gives us the weights assigned to each word(10000 total) in our vacabulary\n",
        "##in the 16 dimensional embedding space.\n",
        "##You can view this embedding dimensions in http://projector.tensorflow.org/\n",
        "##Upload the tsv files to check it out\n",
        "\n",
        "\n",
        "import io\n",
        "\n",
        "out_v=io.open(\"vec.tsv\",'w',encoding='utf-8')\n",
        "out_m=io.open(\"meta.tsv\",'w',encoding='utf-8')\n",
        "\n",
        "for x in range(1,vocab_size):\n",
        "  word=reverse_word_index[x]\n",
        "  embeddings=weights[x]\n",
        "  out_m.write(word+\"\\n\")\n",
        "  out_v.write('\\t'.join([str(x) for x in embeddings]) + \"\\n\")\n",
        "out_v.close()\n",
        "out_m.close()  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohN93gKxCnB7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "  from google.colab import files\n",
        "except ImportError:\n",
        "  pass\n",
        "else:\n",
        "  files.download('vec.tsv')\n",
        "  files.download('meta.tsv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8EdCcW2LCtft",
        "colab_type": "code",
        "outputId": "0226e664-171b-49db-d0c3-a83dadf155e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sentence = \"I really think this is amazing. honest.\"\n",
        "sequence = tokenizer.texts_to_sequences([sentence)\n",
        "print(sequence)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[11, 68, 105, 12, 7, 491, 1216]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxh_7lQIC0Bi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "encoder = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
        "corpus_generator, target_vocab_size=2**15)\n",
        "encoder.save_to_file(vocab_filename)\n",
        "\n",
        "# Load\n",
        "encoder = tfds.features.text.SubwordTextEncoder.load_from_file(vocab_filename)\n",
        "ids = encoder.encode(\"hello world\")\n",
        "text = encoder.decode([1, 2, 3, 4])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DVEx-38NlOX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}